import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

df = pd.read_csv('mnist_train.csv')

df_2=df.loc[df['label'].isin([8,9])]

df_2=df_2.reset_index(drop=True)

df_2.to_csv(r'C:\Users\dell\YandexDisk\Ticaret_dersler\ENM341\mnist_train89.csv', index = None, header=True)


test = pd.read_csv('mnist_test.csv')
    
test_2=test.loc[test['label'].isin([8,9])]

test_2=test_2.reset_index(drop=True)

test_2.to_csv(r'C:\Users\dell\YandexDisk\Ticaret_dersler\ENM341\mnist_test89.csv', index = None, header=True)



xtr = df.iloc[:,1:]
ytr = df.iloc[:,0]

xtst = test.iloc[:,1:]
ytst = test.iloc[:,0]


#decision tree
classifier = DecisionTreeClassifier()
classifier.fit(xtr,ytr)

classifier.score(xtr,ytr)

from sklearn.metrics import accuracy_score
pred = classifier.predict(xtst)
accuracy_score(pred,ytst)

#Ensemble

#Random Forest  -  Combination of various Decision tree
rf = RandomForestClassifier(n_estimators=100)
rf.fit(xtr,ytr)

print('Training Set RF=>',rf.score(xtr,ytr))
print('Test Set RF=>',rf.score(xtst,ytst))

# Bagging

bg = BaggingClassifier(DecisionTreeClassifier(),max_samples=0.2, max_features=1.0, n_estimators=20)
bg.fit(xtr,ytr)

print('Training Set BG=>',bg.score(xtr,ytr))
print('Test Set BG=>',bg.score(xtst,ytst))

#Boosting
ad = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=0.01)
ad.fit(xtr,ytr)

ad.score(xtst,ytst)
ad.score(xtr,ytr)

print('Training Set AB=>',ad.score(xtr,ytr))
print('Test Set AB=>',ad.score(xtst,ytst))

# Voting Classifier - Multiple model ensemble
lr = LogisticRegression()
tree = DecisionTreeClassifier()
svm = SVC(kernel='poly',degree=2)

evc = VotingClassifier( estimators= [('lr',lr),('tree',tree),('svm',svm)], voting = 'hard')
evc.fit(xtr.iloc[1:4000],ytr.iloc[1:4000])
evc.score(xtst,ytst)
